return(list('f' = obj_at_theta, 'theta' = original_theta, 'iter' = i, 'g' = grad_at_theta, 'Hi' = inv_hess))
}
warning('Hessian is not positive definite at convergence.\n')
if(hess_is_inv){
return(list('f' = obj_at_theta, 'theta' = original_theta, 'iter' = i, 'g' = grad_at_theta, 'Hi' = inv_hess_before_perturb))
}
return(list('f' = obj_at_theta, 'theta' = original_theta, 'iter' = i, 'g' = grad_at_theta, 'Hi' = "Hessian is not invertible"))
}
theta <- theta_hat
i <- i + 1
}
warning("Maximum iterations reached without convergence \n")
if(hess_is_pd){
return(list('f' = obj_at_theta, 'theta' = original_theta, 'iter' = maxit, 'g' = grad_at_theta, 'Hi' = inv_hess))
}
warning('Hessian is not positive definite at max iterations.\n')
if(hess_is_inv){
return(list('f' = obj_at_theta, 'theta' = original_theta, 'iter' = maxit, 'g' = grad_at_theta, 'Hi' = inv_hess_before_perturb))
}
return(list('f' = obj_at_theta, 'theta' = original_theta, 'iter' = maxit, 'g' = grad_at_theta, 'Hi' = "Hessian is not invertible"))
}
rb <- function(th) {
0.1*th ^ 3 + 0.1*th ^ 2
}
gb <- function(th) {
0.3*th^2 + 0.2*th
}
hb <- function(th) {
0.6*th + 0.2
}
theta <- c(10, 10)
test <- newt(theta, rb, gb, hb, fscale = 0.5)
debug(newt)
newt(theta, rb, gb, hb, fscale = 0.5)
undebug(newt)
rb <- function(th) {
0.1*th ^ 3 + 0.1*th ^ 2
}
gb <- function(th) {
0.3*th^2 + 0.2*th
}
hb <- function(th) {
0.6*th + 0.2
}
theta <- c(1, 1)
test <- newt(theta, rb, gb, hb, fscale = 0.5)
debug(newt)
newt(theta, rb, gb, hb, fscale = 0.5)
undebug(newt)
rb <- function(th) {
0.1*th ^ 3 + 0.1*th ^ 2
}
gb <- function(th) {
0.3*th^2 + 0.2*th
}
hb <- function(th) {
0.6*th + 0.2
}
theta <- c(1)
test <- newt(theta, rb, gb, hb, fscale = 0.5)
print(test)
rb <- function(th) {
0.1*th ^ 3 + 0.1*th ^ 2
}
gb <- function(th) {
0.3*th^2 + 0.2*th
}
hb <- function(th) {
0.6*th + 0.2
}
theta <- c(10)
test <- newt(theta, rb, gb, hb, fscale = 0.5)
print(test)
rb <- function(th) {
0.1*th ^ 3 + 0.1*th ^ 2
}
gb <- function(th) {
0.3*th^2 + 0.2*th
}
hb <- function(th) {
0.6*th + 0.2
}
theta <- c(0.7)
test <- newt(theta, rb, gb, hb, fscale = 0.5)
print(test)
debug(newt)
newt(theta, rb, gb, hb, fscale = 0.5)
undebug(newt)
rb <- function(th) {
-0.1*th ^ 3 - 0.1*th ^ 2
}
gb <- function(th) {
-0.3*th^2 - 0.2*th
}
hb <- function(th) {
-0.6*th - 0.2
}
theta <- c(-0.7)
test <- newt(theta, rb, gb, hb, fscale = 0.5)
print(test)
debug(newt)
newt(theta, rb, gb, hb, fscale = 0.5)
undebug(newt)
rb <- function(th) {
-0.1*th ^ 3 - 0.1*th ^ 2
}
gb <- function(th) {
-0.3*th^2 - 0.2*th
}
hb <- function(th) {
-0.6*th - 0.2
}
theta <- c(-10)
test <- newt(theta, rb, gb, hb, fscale = 0.5)
print(test)
rb <- function(th) {
-0.1*th ^ 3 + 0.1*th ^ 2
}
gb <- function(th) {
-0.3*th^2 + 0.2*th
}
hb <- function(th) {
-0.6*th + 0.2
}
theta <- c(-1)
test <- newt(theta, rb, gb, hb, fscale = 0.5)
print(test)
test <- -0.5
eigen(test)
hess_at_theta = -0.5
eig_hess <- eigen(hess_at_theta) # Computes the Eigen-decomposition on the
eig_hess <- eigen(hess_at_theta) # Computes the Eigen-decomposition on the
# Hessian matrix.
lambdas <- eig_hess$values # Identifies the eigenvalue matrix of the
lambdas
# Hessian.
U <- eig_hess$vectors # Identifies the matrix of eigenvectors of the Hessian
U
# matrix or the perturbed Hessian matrix.
if(0 %in% lambdas){ # If there is an eigenvalue of 0.
hess_is_inv <- FALSE # Hessian is not invertible.
} else { # If all eigenvalues are non-zero.
hess_is_inv <- TRUE # Hessian is invertible.
inv_hess_before_perturb <- U %*% (diag(1 / lambdas)) %*% t(U) # Calculates
# the inverse of Hessian by Eigen-decomposition.
}
diag(1/ lambdas)
1 / lambdas
hess_at_theta = -0.5
eig_hess <- eigen(hess_at_theta) # Computes the Eigen-decomposition on the
# Hessian matrix.
lambdas <- eig_hess$values # Identifies the eigenvalue matrix of the
if (length(lambdas) == 1){
lambdas = matrix(lambdas)
}
# Hessian.
U <- eig_hess$vectors # Identifies the matrix of eigenvectors of the Hessian
inv_hess_before_perturb <- U %*% (diag(1 / lambdas)) %*% t(U) # Calculates
inv_hess_before_perturb
rb <- function(th) {
1 / th
}
gb <- function(th) {
- 1 / th^2
}
hb <- function(th) {
2 / th^3
}
theta <- c(0)
test <- newt(theta, rb, gb, hb, fscale = 0.5)
rb <- function(th) {
sqrt(th)
}
gb <- function(th) {
- 1 / th^2
}
hb <- function(th) {
2 / th^3
}
theta <- c(-1)
test <- newt(theta, rb, gb, hb, fscale = 0.5)
hess_with_finite_differencing <- function(current_theta, grad, eps){
# This function uses finite differencing to approximate the hessian matrix
# of an objective function with respect to a supplied parameter vector of
# thetas.
#
# Inputs:
# current_theta - a parameter vector that the hessian will be evaluated at
# grad - the gradient function. Returns the gradient vector of the objective
# w.r.t. the elements of parameter vector
#
# Returns:
# a Hessian matrix approximated with finite differencing
p <- length(current_theta) # number of parameters in theta
hess <- matrix(0,p,p) # initialize hessian approximation
for (i in 1:p){ # loop over parameters
change_vector <- matrix(0,p)
change_vector[i] <- eps / 2 # used to change one parameter of theta
# gradient evalauted at current theta - epsilon / 2
lower_grad <- grad(current_theta - change_vector)
# gradient evaluated at current theta + epsilon / 2
upper_grad <- grad(current_theta + change_vector)
hess[i,] <- (upper_grad - lower_grad) / eps # store finite approximation
}
return(hess)
}
newt <- function(theta, func, grad, hess = NULL, ..., tol = 1e-8, fscale = 1,
maxit = 100, max.half = 20, eps = 1e-6){
# Code to implement Newton's ("newt") method for function minimisation.
# The newt function was to be set up in a way where it would operate in a
# similar way to the functions nlm and optim.
#
# The newt function arguments are:
# newt(theta,func,grad,hess=NULL,...,tol=1e-8,fscale=1,maxit=100,max.half=20,
# eps# =1e-6)
#
# Where 'theta' is the vector of initial values for the optimisation
# parameters;
# 'func' is the objective function to be minimised;
# 'grad' is the gradient function;
# 'hess' is the Hessian matrix;
# '...' are arguments of 'func', 'grad', and 'hess';
# 'tol' is the convergence tolerance;
# 'fscale' is an estimate of the value of the objective function near the
# optimum;
# 'maxit' is the limit of newt iterations to try before stopping.
# 'max.half' denotes the restriction on the number of times a step should be
# halved before concluding that the step was not able to improve the
# objective;
# and 'eps' is the finite difference intervals to be used when a Hessian
# function is not supplied.
#
# The constructed newt function returns the optimised/minimised value of the
# objective function, the value of 'theta' at the minimum, the number of
# iterations it took to reach the minimum, the gradient at the minimum, and
# the inverse of the Hessian matrix at the minimum, while at the same time,
# issues errors and warnings in the following scenarios:
#
# 1. Where the objective or derivatives are not finite at the initial theta;
# 2. Where the step fails to bring down the objective after reaching
#    'max.half' number of step halvings.
# 3. Where 'maxit' is reached without convergence*.
# 4. Where the Hessian is not positive definite at convergence*.
#
# *convergence is assessed by checking whether all absolute values of the
# gradient vector is less than the convergence tolerance, 'tol', multiplied by
# the absolute value of the objective function plus the estimate value of the
# objective function near the optimum, 'fscale'.
# If no hessian matrix function is supplied, default to finite differencing
hess.supplied <- TRUE # Default to assume hessian is supplied
if(is.null(hess)){ # If hessian is not supplied
hess.supplied <- FALSE # Set hessian to not supplied
}
obj_at_theta <- func(theta) # Evaluate objective function at initial theta
grad_at_theta <- grad(theta) # Evaluate gradient at initial theta
if(hess.supplied){ # Checks whether hessian is supplied, if so,
hess_at_theta <- hess(theta) # evaluate hessian at inital theta
} else { #otherwise,
# approximate hessian matrix by finite differencing
hess_at_theta <- hess_with_finite_differencing(theta, grad, eps)
}
# Collate obj, gradient, and hessian values at intial theta into 1 vector,
obj_and_derivatives <- c(obj_at_theta, grad_at_theta, hess_at_theta)
# and check that all values are finite.
# If not finite, stop the function and throw an error message
if (!all(is.finite(obj_and_derivatives)) ){
stop("Objective Funciton or Derivatives Not Finite at Initial Theta")
}
i <- 0 # Start iterations at 0
while(i < maxit){ # Track the number of iterations up to the max allowance.
original_theta <- theta # store value of theta before the current loop runs
# Evaluates the corresponding function value at theta
obj_at_theta <- func(theta)
# Evaluates the corresponding gradient value at theta
grad_at_theta <- grad(theta)
# Reckon the Hessian.
if(hess.supplied){ # If a user specifies the Hessian matrix.
hess_at_theta <- hess(theta) # Evaluates the value at theta.
} else { # If the user doesn't specify the Hessian matrix
# Applys the finite differencing function, and approximates the hessian
# at theta
hess_at_theta <- hess_with_finite_differencing(theta, grad, eps)
}
# Make a trial of constructing Cholesky decomposition to the Hessian
chol_of_hess <- try(chol(hess_at_theta), silent = TRUE)
# Check if the cholesky decomposition is possible
if(all(class(chol_of_hess) != 'try-error')){
hess_is_pd <- TRUE # Determines that the Hessian is positive definite
hess_is_inv <- TRUE # Determines that the Hessian is invertible
# Reckons the inverse of Hessian from Cholesky decomposition
inv_hess <- chol2inv(chol_of_hess)
} else { # Cholesky decomposition not possible --> hessian is not PD
hess_is_pd <- FALSE # Hessian cannot be positive definite
# Computes the Eigen-decomposition of the Hessian matrix
eig_hess <- eigen(hess_at_theta)
lambdas <- eig_hess$values # Identifies the eigenvalue matrix of the
if (length(lambdas) == 1){ # check for special case when hessian is 1x1
lambdas = matrix(lambdas) # coerce lambda from a scalar to a matrix
}
U <- eig_hess$vectors # Identifies matrix of eigenvectors of the Hessian
# If there is an eigenvalue of 0, hessian is not invertible
if(0 %in% lambdas){
hess_is_inv <- FALSE # Hessian is not invertible.
} else { # If all eigenvalues are non-zero
hess_is_inv <- TRUE # Hessian is invertible.
# Calculates the inverse of Hessian by Eigen-decomposition.
inv_hess_before_perturb <- U %*% (diag(1 / lambdas)) %*% t(U)
}
# Perturb the Hessian by the absolute value of the smallest eigenvalue,
# plus one
pert_hess_at_theta <- hess_at_theta + (-(min(lambdas)) + 1) *
diag(dim(as.matrix(hess_at_theta))[1])
# Calculates the inverse of the perturbed Hessian
inv_hess <- chol2inv(chol(pert_hess_at_theta))
}
# Determine whether the convergence condition is met.
if (all(abs(grad_at_theta) < tol * (abs(obj_at_theta) + fscale))){
# If all elements in the gradient less than tolerance multiples of
# absolute value of objective function plus a rough estimate of the
# magnitude of the objective function near the optimum, the convergent
# condition is met.
# If the Hessian is positive definite, returns a list of
# the following variables: optimal value, optimal solution, number of
# iterations, gradient at optimal solution, and inverse of Hessian
# matrix at optimal solution.
if(hess_is_pd){
return(list('f' = obj_at_theta, 'theta' = theta, 'iter' = i,
'g' = grad_at_theta, 'Hi' = inv_hess))
}
# However, if Hessian is not positive definite at convergence, give a
# warning first, and return values anyways
warning('Hessian is not positive definite at convergence.\n')
# If Hessian is invertible, return the similar list, but
# return the inverse of the Hessian as the one before perturbation
if(hess_is_inv){
return(list('f' = obj_at_theta, 'theta' = theta, 'iter' = i,
'g' = grad_at_theta, 'Hi' = inv_hess_before_perturb))
}
# If Hessian is not invertible, return a message indicating that it is
# not invertible
return(list('f' = obj_at_theta, 'theta' = theta, 'iter' = i,
'g' = grad_at_theta, 'Hi' = "Hessian is not invertible"))
}
descent_direction <- as.vector(-inv_hess %*% grad_at_theta)
stepsize <- 1 # Tentatively use the full descent direction
halves <- 0 # Initialise half step variable
# Update theta by moving it to a step size multiple of descent direction
theta_hat <- theta + stepsize * descent_direction
suppressWarnings( # suppress default NaN warnings if they occur
# Keep attempting to halve the step size as long as the maximum number of
# halves have not been tried, and any of the following conditions are met:
# 1. Obj function increases at proposed theta compared to current theta
# 2. Obj function not finite at proposed theta
while((obj_at_theta < func(theta_hat) && halves <= max.half) |
(!(is.finite(func(theta_hat))) && halves <= max.half)){
stepsize <- stepsize / 2
theta_hat <- theta +  stepsize * descent_direction # halve step size
halves <- halves + 1 # increment halve count
}
)
if(halves == (max.half + 1)){ # check if maximum half steps were exceeded
warning("Maximum number of step halvings reached \n")
if(hess_is_pd){ # check if hessian is positive definite
# If the hessian is positive definite, return all values without
# additional warnings
return(list('f' = obj_at_theta, 'theta' = original_theta, 'iter' = i,
'g' = grad_at_theta, 'Hi' = inv_hess))
}
# Warn user that Hessian is not positive definite
warning('Hessian is not positive definite at max halve steps.\n')
if(hess_is_inv){ # check if hessian is invertible
# return all values if hessian is invertible
return(list('f' = obj_at_theta, 'theta' = original_theta, 'iter' = i,
'g' = grad_at_theta, 'Hi' = inv_hess_before_perturb))
}
# not possible to return inverse hessian if hessian is not invertible
return(list('f' = obj_at_theta, 'theta' = original_theta, 'iter' = i,
'g' = grad_at_theta, 'Hi' = "Hessian is not invertible"))
}
theta <- theta_hat # update theta
i <- i + 1 # increment iteration count
}
# If this portion of the code is reached, that means that the maximum number
# of iterations has run and convergence was not reached
warning("Maximum iterations reached without convergence \n")
if(hess_is_pd){ # check if hessian is positive definite
# If the hessian is positive definite, return all values without additional
# warnings
return(list('f' = obj_at_theta, 'theta' = original_theta,
'iter' = maxit, 'g' = grad_at_theta, 'Hi' = inv_hess))
}
# Warn user that Hessian is not positive definite
warning('Hessian is not positive definite at max iterations.\n')
if(hess_is_inv){ # check if hessian is invertible
# return all values if hessian is invertible
return(list('f' = obj_at_theta, 'theta' = original_theta, 'iter' = maxit,
'g' = grad_at_theta, 'Hi' = inv_hess_before_perturb))
}
# not possible to return inverse hessian if hessian is not invertible
return(list('f' = obj_at_theta, 'theta' = original_theta, 'iter' = maxit,
'g' = grad_at_theta, 'Hi' = "Hessian is not invertible"))
}
## Typical function with a minimum with no special attributes
rb <- function(th,k = 2) {
k * (th[2] - th[1] ^ 2) ^ 2 + (1 - th[1]) ^ 2
}
gb <- function(th,k = 2) {
c(-2 * (1 - th[1]) - k * 4 * th[1] * (th[2] - th[1] ^ 2),k * 2 * (th[2] - th[1] ^ 2))
}
hb <- function(th,k = 2) {
h <- matrix(0,2,2)
h[1,1] <- 2 - k * 2 * (2 * (th[2] - th[1] ^ 2) - 4 * th[1] ^ 2)
h[2,2] <- 2 * k
h[1,2] <- h[2,1] <- -4 * k * th[1]
h
}
theta <- c(10,10)
test <- newt(theta, rb, gb, k = 2)
print(test)
## Typical function with a minimum with no special attributes
rb <- function(th,k = 2) {
k * (th[2] - th[1] ^ 2) ^ 2 + (1 - th[1]) ^ 2
}
gb <- function(th,k = 2) {
c(-2 * (1 - th[1]) - k * 4 * th[1] * (th[2] - th[1] ^ 2),k * 2 * (th[2] - th[1] ^ 2))
}
hb <- function(th,k = 2) {
h <- matrix(0,2,2)
h[1,1] <- 2 - k * 2 * (2 * (th[2] - th[1] ^ 2) - 4 * th[1] ^ 2)
h[2,2] <- 2 * k
h[1,2] <- h[2,1] <- -4 * k * th[1]
h
}
theta <- c(0,0)
test <- newt(theta, rb, gb, bh, k = 2)
## Typical function with a minimum with no special attributes
rb <- function(th,k = 2) {
k * (th[2] - th[1] ^ 2) ^ 2 + (1 - th[1]) ^ 2
}
gb <- function(th,k = 2) {
c(-2 * (1 - th[1]) - k * 4 * th[1] * (th[2] - th[1] ^ 2),k * 2 * (th[2] - th[1] ^ 2))
}
hb <- function(th,k = 2) {
h <- matrix(0,2,2)
h[1,1] <- 2 - k * 2 * (2 * (th[2] - th[1] ^ 2) - 4 * th[1] ^ 2)
h[2,2] <- 2 * k
h[1,2] <- h[2,1] <- -4 * k * th[1]
h
}
theta <- c(0,0)
test <- newt(theta, rb, gb, hb, k = 2)
print(test)
## Linear function y = x, using finite differencing
rb <- function(th) {
th
}
gb <- function(th) {
1
}
hb <- function(th) {
0
}
theta = c(10)
test <- newt(theta, rb, gb, fscale = 0.5)
print(test)
## A function of a cone
rb <- function(th) {
th[1] ^ 2 + th[2] ^ 2
}
gb <- function(th) {
c(th[1] * 2, th[2] * 2)
}
hb <- function(th) {
matrix(c(2,0,0,2),2,2)
}
theta <- c(100, 1000)
test <- newt(theta, rb, gb, fscale = 0.5)
print(test)
rb <- function(th) {
sqrt(th)
}
gb <- function(th) {
- 1 / th^2
}
hb <- function(th) {
2 / th^3
}
theta <- c(-1)
test <- newt(theta, rb, gb, hb, fscale = 0.5)
## Typical function with a minimum with no special attributes
rb <- function(th,k = 2) {
k * (th[2] - th[1] ^ 2) ^ 2 + (1 - th[1]) ^ 2
}
gb <- function(th,k = 2) {
c(-2 * (1 - th[1]) - k * 4 * th[1] * (th[2] - th[1] ^ 2),k * 2 * (th[2] - th[1] ^ 2))
}
hb <- function(th,k = 2) {
h <- matrix(0,2,2)
h[1,1] <- 2 - k * 2 * (2 * (th[2] - th[1] ^ 2) - 4 * th[1] ^ 2)
h[2,2] <- 2 * k
h[1,2] <- h[2,1] <- -4 * k * th[1]
h
}
theta <- c(10,10)
test <- newt(theta, rb, gb, hb, k = 2, max.half = 2)
print(test)
knitr::opts_chunk$set(echo = TRUE)
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
