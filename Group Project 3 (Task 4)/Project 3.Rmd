---
title: "Project 1"
author: "Will Graham; Richelle Lee; Robin Lin"
date: '2022-10-06'
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
newt <- function(theta, func, grad, hess = NULL, ..., tol, fscale, maxit, max.half, eps){
  
  obj_at_theta <- func(theta)
  grad_at_theta <- grad(theta)
  if(is.null(hess)){
    finite_diff_grad <- function(delta, x_grad, current_theta){
      grad_diff <- grad(current_theta + delta/2) - grad(current_theta - delta/2)
      grad_diff <- matrix(grad_diff, len(grad_diff))
    }
  }

result <- lapply(test, FUN = func, divisor)
  }
  
  hess_at_theta <- hess(theta)
  inf <- c(Inf, -Inf)
  
  if(obj_at_theta %in% inf | grad_at_theta %in% inf){ # Check that the objective function is finite at theta
    
  } 
    
  
  
  # Check maxit
  # Save the value of D at theta (alpha)
  # Calculate minimum of approximation function at theta
    ## Calculate the gradient and hessian matrix of the obj func
    ## Check if the gradient converged
      ### If yes:
        #### If hessian is not positive definite, give a warning, and retrun values
        #### If hessian is postive definite, return values
    ## Calculate eigen decomp on the hessian
      ### Check lambdas for positivity, if not, perturb hessian matrix and recalculate eigen decomp
    ## Get inverse of hessian by taking reciprocal of lambda
    ## Return theta that minimises the approximation funct
  
  # Set stepsize = theta hat - theta
  # Compare value of D at theta hat (beta) to alpha:
    ## Case 1: alpha less than beta
      ### try dividing stepsize by 2 until valid obj funct at theta:
      ### set theta hat = theta + stepsize*0.5
  # rerun this loop
  # if runs max.half number of times without improving, stop
  
  
  
}

```

